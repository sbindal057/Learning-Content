


import numpy as np

# Plotting library
from matplotlib import pyplot

# Optimization module in scipy
from scipy import optimize

# will be used to load MATLAB mat datafile format
from scipy.io import loadmat

data = loadmat("C:/Users/Shweta Bindal/Desktop/Shweta_Bindal190123057/ex5data1.mat")

# Extract train, test, validation data from dictionary
# and also convert y's form 2-D matrix (MATLAB format) to a numpy vector
X, y = data['X'], data['y'][:, 0]
Xtest, ytest = data['Xtest'], data['ytest'][:, 0]
Xval, yval = data['Xval'], data['yval'][:, 0]



# m = Number of examples
m = y.size

# Plot training data
pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1)
pyplot.xlabel('Change in water level (x)')
pyplot.ylabel('Water flowing out of the dam (y)');

pyplot.show()



def linearRegCostFunction(X, y, theta, lambda_):
    
    if theta.ndim==1:
        J = np.dot(np.dot(X,theta)-y,np.dot(X,theta)-y) + lambda_*(np.dot(theta,theta) - theta[0]*theta[0])
        
        grad = np.zeros(theta.shape)
        grad[0] = np.sum(np.dot(X,theta)-y)
        for i in range (1,theta.size):
            grad[i] = np.dot(np.dot(X,theta)-y,X[:,i]) + lambda_*theta[i]
    
        
        
    J=J/(2*m)
    grad = grad/m
    
    return J,grad
    
    
    
   


theta = np.array([1, 1])
J,_ = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)

print('Cost at theta = [1, 1]:\t   %f ' % J)
print('This value should be about 303.993192)\n' % J)

theta = np.array([1, 1])
J, grad = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)

print('Gradient at theta = [1, 1]:  [{:.6f}, {:.6f}] '.format(*grad))
print(' (this value should be about [-15.303016, 598.250744])\n')


def trainLinearReg(linearRegCostFunction, X, y, lambda_=0.0, maxiter=200):
    
    # Initialize Theta
    initial_theta = np.zeros(X.shape[1])

    # Create "short hand" for the cost function to be minimized
    costFunction = lambda t: linearRegCostFunction(X, y, t, lambda_)

    # Now, costFunction is a function that takes in only one argument
    options = {'maxiter': maxiter}

    # Minimize using scipy
    res = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)
    return res.x
# add a columns of ones for the y-intercept
X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
theta = trainLinearReg(linearRegCostFunction, X_aug, y, lambda_=0)

#  Plot fit over the data
pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1.5)
pyplot.xlabel('Change in water level (x)')
pyplot.ylabel('Water flowing out of the dam (y)')
pyplot.plot(X, np.dot(X_aug, theta), '--', lw=2)
pyplot.show()



def learningCurve(X, y, Xval, yval, lambda_=0):
    
    m = y.size

    # You need to return these values correctly
    error_train = np.zeros(m)
    error_val   = np.zeros(m)

    
    for i in range(1,y.size):
        theta = trainLinearReg(linearRegCostFunction, X[:i,:], y[:i], lambda_=0)
    
        
    
        error_train[i-1] = np.dot(np.dot(X[:i,:],theta)-y[:i],np.dot(X[:i,:],theta)-y[:i])/(2*m)
        
        error_val[i-1] = np.dot(np.dot(Xval,theta)-yval,np.dot(Xval,theta)-yval)/(2*m)
        
    return error_train,error_val

X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)
Xval_aug = np.concatenate([np.ones((yval.size, 1)), Xval], axis=1)
error_train, error_val = learningCurve(X_aug, y, Xval_aug, yval, lambda_=0)

pyplot.plot(np.arange(1, m+1), error_train, np.arange(1, m+1), error_val, lw=2)
pyplot.title('Learning curve for linear regression')
pyplot.legend(['Train', 'Cross Validation'])
pyplot.xlabel('Number of training examples')
pyplot.ylabel('Error')
pyplot.axis([0, 13, 0, 150])
pyplot.show()

print('# Training Examples\tTrain Error\tCross Validation Error')
for i in range(m):
    print('  \t%d\t\t%f\t%f' % (i+1, error_train[i], error_val[i]))
    
    
def polyFeatures(X, p):
    X_poly = np.zeros((X.shape[0], p))
    
    for i in range(0,X.shape[0]):
        for j in range(1,p+1):
            X_poly[i,j-1] = pow(X[i,0],j)
            
    return X_poly


def featureNormalize(X):
    X_norm = np.zeros(X.shape)
    for i in range(0,X.shape[1]):
        mu = np.mean(X[:,i])
        X_norm[:,i] = X[:,i] - mu

        sigma = np.std(X[:,i])
        
        X_norm[:,i] /= sigma
    return X_norm


p = 8

# Map X onto Polynomial Features and Normalize
X_poly = polyFeatures(X, p)

X_poly = featureNormalize(X_poly)
X_poly = np.concatenate([np.ones((m, 1)), X_poly], axis=1)

# Map X_poly_test and normalize (using mu and sigma)
X_poly_test = polyFeatures(Xtest, p)

X_poly_test = featureNormalize(X_poly_test)

X_poly_test = np.concatenate([np.ones((ytest.size, 1)), X_poly_test], axis=1)

# Map X_poly_val and normalize (using mu and sigma)
X_poly_val = polyFeatures(Xval, p)

X_poly_val = featureNormalize(X_poly_val)

X_poly_val = np.concatenate([np.ones((yval.size, 1)), X_poly_val], axis=1)

print('Normalized Training Example 1:')
X_poly[0, :]

def plotFit(polyFeatures, min_x, max_x, theta, p):
   
    # We plot a range slightly bigger than the min and max values to get
    # an idea of how the fit will vary outside the range of the data points
    x = np.arange(min_x - 15, max_x + 25, 0.05).reshape(-1, 1)

    # Map the X values
    X_poly = polyFeatures(x, p)
    X_poly = featureNormalize(X_poly)
    

    # Add ones
    X_poly = np.concatenate([np.ones((x.shape[0], 1)), X_poly], axis=1)

    # Plot
    pyplot.plot(x, np.dot(X_poly, theta), '--', lw=2)
        
lambda_ = 00
theta = trainLinearReg(linearRegCostFunction, X_poly, y,
                             lambda_=lambda_, maxiter=55)

# Plot training data and fit
pyplot.plot(X, y, 'ro', ms=10, mew=1.5, mec='k')

plotFit(polyFeatures, np.min(X), np.max(X),  theta, p)


pyplot.xlabel('Change in water level (x)')
pyplot.ylabel('Water flowing out of the dam (y)')
pyplot.title('Polynomial Regression Fit (lambda = %f)' % lambda_)
pyplot.ylim([-20, 50])

pyplot.figure()
error_train, error_val = learningCurve(X_poly, y, X_poly_val, yval, lambda_)
pyplot.plot(np.arange(1, 1+m), error_train, np.arange(1, 1+m), error_val)

pyplot.title('Polynomial Regression Learning Curve (lambda = %f)' % lambda_)
pyplot.xlabel('Number of training examples')
pyplot.ylabel('Error')
pyplot.axis([0, 13, 0, 100])
pyplot.legend(['Train', 'Cross Validation'])
pyplot.show()

print('Polynomial Regression (lambda = %f)\n' % lambda_)
print('# Training Examples\tTrain Error\tCross Validation Error')
for i in range(m):
    print('  \t%d\t\t%f\t%f' % (i+1, error_train[i], error_val[i]))
    

def validationCurve(X, y, Xval, yval):
     # Selected values of lambda (you should not change this)
    lambda_vec = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])

    # You need to return these variables correctly.
    error_train = np.zeros(lambda_vec.size)
    error_val = np.zeros(lambda_vec.size)
    
    for i in range(0,lambda_vec.size):
        theta = trainLinearReg(linearRegCostFunction, X[:i,:], y[:i], lambda_=0)
    
        
    
        error_train[i] = np.dot(np.dot(X,theta)-y,np.dot(X,theta)-y)/(2*m)
        
        error_val[i] = np.dot(np.dot(Xval,theta)-yval,np.dot(Xval,theta)-yval)/(2*m)

    



   
    return lambda_vec, error_train, error_val

lambda_vec, error_train, error_val = validationCurve(X_poly, y, X_poly_val, yval)

pyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)
pyplot.legend(['Train', 'Cross Validation'])
pyplot.xlabel('lambda')
pyplot.ylabel('Error')

print('lambda\t\tTrain Error\tValidation Error')
for i in range(len(lambda_vec)):
    print(' %f\t%f\t%f' % (lambda_vec[i], error_train[i], error_val[i]))

    
