import numpy as np
import matplotlib.pyplot as plt


data = np.loadtxt("https://raw.githubusercontent.com/IITGuwahati-AI/Learning-Content/master/Phase%203%20-%202020%20(Summer)/Week%203(Apr%2013%20-%20Apr%2018)/Exercise2/Data/ex2data2.txt", delimiter=',')
x = data[:, :2]
y = data[:, 2]

pos = y == 1
neg = y == 0

plt.plot(x[pos, 0], x[pos, 1], 'k*', lw=2, ms=10)
plt.plot(x[neg, 0], x[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)

plt.legend(['Admitted', 'Not admitted'])

plt.xlabel('Microchip Test 1')
plt.ylabel('Microchip Test 2')


plt.legend(['y = 1', 'y = 0'], loc='upper right')
a=np.array([x[:,0],x[:,1],x[:,0]*x[:,0]])

def map_feature(x1, x2):
    '''
    Maps the two input features to quadratic features.
    Returns a new feature array with more features, comprising of
    X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, etc...
    Inputs X1, X2 must be the same size
    '''
    x1.shape = (x1.size, 1)
    x2.shape = (x2.size, 1)
    degree = 6
    out = np.ones(shape=(x1[:, 0].size, 1))

    m, n = out.shape

    for i in range(1, degree + 1):
        for j in range(i + 1):
            r = (x1 ** (i - j)) * (x2 ** j)
            out = np.append(out,r, axis=1)

    return out

c=map_feature(x[:,0],x[:,1])



def sigmoid(z):

    return 1/(1+np.exp(-z))
m, n = x.shape



def computeCost(theta,y,c,lambda_):
    l=np.dot(c,theta)
    k=lambda_*np.dot(theta,theta)/2
    for i in range(y.size):
        k=k+(-y[i]*np.log(sigmoid(l)[i])-(1-y[i])*np.log(1-sigmoid(l)[i]))
    j=k/m
    
    
    d=sigmoid(l)-y
    p=np.zeros(c.shape[1])
    for i in range(c.shape[1]):
        p[i]=(np.dot(d,c[:,i])/m) +(lambda_*theta[i]/m)
    
    return j,p


initial_theta = np.zeros(c.shape[1])


lambda_ = 1


cost, grad = computeCost(initial_theta,y,c,lambda_)

print('Cost at initial theta (zeros): {:.3f}'.format(cost))
print('Expected cost (approx)       : 0.693\n')

print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n')



test_theta = np.ones(c.shape[1])
cost, grad = computeCost(test_theta,y,c,lambda_)

print('------------------------------\n')
print('Cost at test theta    : {:.2f}'.format(cost))
print('Expected cost (approx): 3.16\n')

print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')



options= {'maxiter': 400}

from scipy.optimize import minimize
res = minimize(computeCost,initial_theta,(y,c,lambda_),jac=True,
                        method='TNC',
                        options=options)

theta = res.x

def plotDecisionBoundary(theta, axes):
    u = np.linspace(-1, 1.5, 50)
    v = np.linspace(-1, 1.5, 50)
    U,V = np.meshgrid(u,v)
   
    U = np.ravel(U)
    V = np.ravel(V)
    Z = np.zeros((len(u) * len(v)))
    
    
    X_poly =  map_feature(U,V)
    Z = X_poly.dot(theta)
    
   
    U = U.reshape((len(u), len(v)))
    V = V.reshape((len(u), len(v)))
    Z = Z.reshape((len(u), len(v)))
    
    cs = axes.contour(U,V,Z,levels=[0],cmap= "Greys_r")
    axes.legend(labels=['good', 'faulty', 'Decision Boundary'])
    return cs


fig, axes = plt.subplots();

plt.plot(x[pos, 0], x[pos, 1], 'k*', lw=2, ms=10)
plt.plot(x[neg, 0], x[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)

plt.legend(['Admitted', 'Not admitted'])

plt.xlabel('Microchip Test 1')
plt.ylabel('Microchip Test 2')


plt.legend(['y = 1', 'y = 0'], loc='upper right')

plotDecisionBoundary(theta,  axes)
